{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52314ed0",
   "metadata": {},
   "source": [
    "# Assignment 2: Python for Text Processing\n",
    "\n",
    "**Submission deadline:** Friday, 31 Oct 2025, 11:55 PM  \n",
    "**Assessment marks:** 35 marks (35% of the total unit assessment)\n",
    "\n",
    "---\n",
    "\n",
    "### Late Submission Penalty\n",
    "\n",
    "Unless a Special Consideration request has been submitted and approved, a 5% penalty (of the total possible mark of the task) will be applied for each day a written report or presentation assessment is not submitted, up until the 7th day (including weekends). After the 7th day, a grade of ‘0’ will be awarded even if the assessment is submitted.\n",
    "\n",
    "> **Example:** If the assignment is worth 8 marks (of the entire unit) and your submission is late by 19 hours (or 23 hours 59 minutes 59 seconds), 0.4 marks (5% of 8 marks) will be deducted. If your submission is late by 24 hours (or 47 hours 59 minutes 59 seconds), 0.8 marks (10% of 8 marks) will be deducted, and so on.\n",
    "\n",
    "The submission time for all uploaded assessments is **11:55 PM**. A **1-hour grace period** is provided for technical concerns.  Apply for [Special Consideration](https://students.mq.edu.au/study/assessment-exams/special-consideration), if you think you should be granted an extended deadline or waive the late submission penalty. You should apply immediately when the situation occurs.\n",
    "\n",
    "---\n",
    "\n",
    "### Academic Integrity\n",
    "\n",
    "All submitted work must be your own. For rules around AI tools, refer to **\"Using Generative AI Tools\" on iLearn**.\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Complete the five tasks below.\n",
    "\n",
    "* Write your code and comments inside this notebook.\n",
    "\n",
    "* Your notebook must include the running outputs of your final code.\n",
    "\n",
    "* **Submit this `.ipynb` file, containing your code and outputs, to iLearn.**\n",
    "\n",
    "---\n",
    "\n",
    "### Assessment\n",
    "\n",
    "-  Marks are based on the correctness of your code, outputs, and coding style.\n",
    "-  A total of **2.5 marks** (0.5 per task) are awarded globally across the assignment for both of the below: (1) runnable codes; (2) good coding style: clean, modular code, meaningful variable names, and good comments.\n",
    "-  If outputs are missing or incorrect, up to **25% of the marks for that task** can be deducted.\n",
    "-  See each task below for the detailed mark breakdown.\n",
    "\n",
    "---\n",
    "\n",
    "### AI Tools Usage Policy\n",
    "\n",
    "\n",
    "In this assignment, we view AI code generators such as copilot, CodeGPT, etc as tools that can help you write code quickly. You are allowed to use these tools, but with some conditions. To understand what you can and what you cannot do, please visit these information pages provided by Macquarie University.\n",
    "\n",
    "- See: [Artificial Intelligence Tools and Academic Integrity in FSE](https://bit.ly/3uxgQP4)\n",
    "\n",
    "If you choose to use these tools, make the following explicit in your submitted file as comments starting with \"Use of AI generators in this assignment\" :\n",
    "\n",
    "- What part of your code is based on the output of such tools,\n",
    "- What tools you used,\n",
    "- What prompts you used to generate the code or text, and\n",
    "- What modifications you made on the generated code or text?\n",
    "\n",
    "This will help us assess your work fairly. \n",
    "\n",
    "**If we observe that you have used an AI generator and you do not give the above information, you may face disciplinary action.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6c533",
   "metadata": {},
   "source": [
    "\n",
    "## Objectives of this assignment\n",
    "\n",
    "In this assignment, you will work on the Quora Question Pairs (QQP) datset detailed below. The first two tasks will help you get familiar with the data, and the remaining requires you to implement deep neural networks.\n",
    "\n",
    "\n",
    "**About the Quora Question Pairs (QQP) Dataset**\n",
    "\n",
    "Description: A large dataset of 400k+ question pairs from Quora, labeled whether they are duplicates (semantically the same) or not. It features informal, noisy text with class imbalance, hard positives (low lexical overlap) and hard negatives (high overlap, different meaning). QQP is practically relevant for deduplicating FAQs, search, and support systems. Working on QQP builds transferable skills, such as text preprocessing, model comparison, threshold tuning, error analysis, and deployment-minded reasoning about real applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4113f16",
   "metadata": {},
   "source": [
    "**Get familiar with the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3b74181",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install datasets    # Install the datasets package to access the dataset\n",
    "# add the packages you used, and specify the verion you installed\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import nltk\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55f743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load QQP\n",
    "ds = load_dataset(\"glue\", \"qqp\")\n",
    "\n",
    "# Use validation set as our test; optionally create a smaller train subset for speed\n",
    "train_ds = ds[\"train\"]\n",
    "eval_ds  = ds[\"validation\"]\n",
    "\n",
    "q1_tr = list(train_ds[\"question1\"])\n",
    "q2_tr = list(train_ds[\"question2\"])\n",
    "y_tr  = np.array(train_ds[\"label\"])\n",
    "\n",
    "q1_te = list(eval_ds[\"question1\"])\n",
    "q2_te = list(eval_ds[\"question2\"])\n",
    "y_te  = np.array(eval_ds[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05743951",
   "metadata": {},
   "source": [
    "### Task 1. What is the top-5 common NOUN in the question1 and question2, respectively? (5 marks)\n",
    "\n",
    "Write codes that returns the top-5 common NOUN in the questions. To find the part of speech, use NLTK's \"Universal\" tag set. You may need to use NLTK's `sent_tokenize` and `word_tokenize` to get words. The function returns a list that is descendingly sorted according to freqency, e.g. [(noun1, 22), (noun2, 10), ...].\n",
    "<!-- To produce the correct results, the function must do this.  -->\n",
    "Hint: The following steps will produce the correct results:\n",
    "\n",
    "- Concatenate all questions together.\n",
    "- Use the NLTK libraries to find the tokens and the stems.\n",
    "- Use NLTK's sentence tokeniser before NLTK's word tokeniser.\n",
    "- Use NLTK's part of speech tagger, using the \"Universal\" tagset.\n",
    "- Use NLTK's `pos_tag_sents` instead of `pos_tag`.\n",
    "\n",
    "Marking Criteria: \n",
    "- 2.5 marks for the correct codes and results of each column, namely question1 and question2 columns.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bb05fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below\n",
    "def get_most_common_nouns(data, most_common_count=5):\n",
    "    word_token = [nltk.word_tokenize(st) for sentance in data for st in nltk.sent_tokenize(sentance)]\n",
    "    tagged_words_list = nltk.pos_tag_sents(word_token, tagset='universal')\n",
    "    tagged_nouns = [tagged_word[0] for tagged_words in tagged_words_list for tagged_word in tagged_words if tagged_word[1] == 'NOUN']\n",
    "    noun_counter = collections.Counter(tagged_nouns)\n",
    "    return noun_counter.most_common(most_common_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dfddf5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_nouns_in_q1 = get_most_common_nouns(q1_tr)\n",
    "most_common_nouns_in_q2 = get_most_common_nouns(q2_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5036716a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most common nouns in question 1: [('India', 11998), ('people', 11342), ('way', 7622), ('Quora', 7595), ('difference', 6641)]\n",
      "Top 5 Most common nouns in question 2: [('India', 12804), ('people', 12095), ('way', 8529), ('Quora', 7889), ('life', 7098)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 Most common nouns in question 1:\", most_common_nouns_in_q1)\n",
    "print(\"Top 5 Most common nouns in question 2:\", most_common_nouns_in_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ac4b3",
   "metadata": {},
   "source": [
    "### Task 2. What are the top-5 common stem 2-grams and non-stem 2-grams for question1 and question2, respectively? (5 marks)\n",
    "\n",
    "Write codes that returns the top-5 most frequent 2-grams (bigrams) of stemmed and non-stemmed tokens along with their normalized frequency from the question1 and question2 columns of the QQP dataset. The output should be in descending order of frequency, **with frequencies normalized by the total number of bigrams (rounded to 4 decimal places)**, e.g., `[(('what', 'is'), 0.0105), (('what', 'are'), 0.0053), ...]`.\n",
    "\n",
    "<!-- To produce the correct results, the function must do this: -->\n",
    "\n",
    "Hint: The following steps will produce the correct results:\n",
    "\n",
    "- Concatenate all questions together.\n",
    "- Use NLTK's sentence tokeniser before NLTK's word tokeniser.\n",
    "- Use the NLTK libraries to find the tokens and the stems.\n",
    "- Use NLTK's Porter stemmer to get the root words.\n",
    "- Round normalized frequency to 4 precision after the decimal point.\n",
    "- When computing bigrams, do not consider words that are in different sentences. For example, if we have this text: `Sentence 1. And sentence 2.` the bigrams are: `('Sentence','1'), ('1','.'), ('.','And'), ('And','sentence')`, etc. Note that the following would not be a valid bigram, since the punctuation mark and the word \"And\" are in different sentences: `('.','And')`.\n",
    "\n",
    "Marking Criteria: \n",
    "- 2.5 marks for the correct codes and restuls of each column, namely question1 and question2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e151460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936c3bf",
   "metadata": {},
   "source": [
    "### Task 3. Naïve Bayes Classifier (5.5 marks)\n",
    "\n",
    "The QQR dataset contains pairs of questions with labels indicating whether the two questions are semantically duplicate (1) or not (0).\n",
    "\n",
    "1. Using a Bag-of-Words representation, train a Naïve Bayes classifier to predict duplicates. (2 marks)\n",
    "\n",
    "1. Report accuracy, precision, and recall on the test set. (1.5 marks)\n",
    "\n",
    "1. Inspect your confusion matrix. Identify one type of error (false positive or false negative) that dominates. Suggest a possible reason for this pattern based on the dataset. (2 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880092f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From now on, you are allowed to use a subset of the dataset which requires less computing resources.\n",
    "# Note that you have to use the same subset for the following coding tasks, which ensure fairness when comparing performance across different models.\n",
    "\n",
    "Ntrain = 1000\n",
    "Ntest = 100\n",
    "ds = load_dataset(\"glue\", \"qqp\")\n",
    "\n",
    "# Use validation set as our test; optionally create a smaller train subset for speed\n",
    "train_ds = ds[\"train\"].select(range(Ntrain))\n",
    "eval_ds  = ds[\"validation\"].select(range(Ntest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8143396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf16e2",
   "metadata": {},
   "source": [
    "### Task 4. Siamese Neural Network (7 marks)\n",
    "\n",
    "You now want to learn semantic similarity directly from the question pairs.\n",
    "\n",
    "1. Design a Siamese Neural Network with two identical LSTM encoders that embed each question. (3 marks)\n",
    "\n",
    "1. Use cosine similarity to classify duplicates, and report accuracy and F1-score. (2 marks)\n",
    "\n",
    "1. Compare your Siamese model to your Naïve Bayes model. Which one handles imbalanced errors (precision vs. recall) better in your results, and why do you think that is? (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b2875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf68de",
   "metadata": {},
   "source": [
    "### Task 5. Transformer-Based Classifier (10 marks)\n",
    "\n",
    "Instead of handcrafted features or LSTMs, you now fine-tune a pre-trained Transformer (e.g., BERT or RoBERTa, etc) for QQP.\n",
    "\n",
    "1. Fine-tune the model for 3 epochs with learning rate 2e-5. (3 marks)\n",
    "\n",
    "1. Report the accuracy, precision, recall, and F1-score. (2 marks)\n",
    "\n",
    "1. Compare your Transformer results with your Siamese model. Did the Transformer improve both precision and recall, or mainly one? What does this suggest about how it captures question meaning? (2 marks)\n",
    "\n",
    "1. Look at one example your Transformer misclassified. Write a short explanation of why the model might have made this mistake. (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309af6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp3420",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
