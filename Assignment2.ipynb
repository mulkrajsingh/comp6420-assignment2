{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52314ed0",
   "metadata": {},
   "source": [
    "# Assignment 2: Python for Text Processing\n",
    "\n",
    "**Submission deadline:** Friday, 31 Oct 2025, 11:55 PM  \n",
    "**Assessment marks:** 35 marks (35% of the total unit assessment)\n",
    "\n",
    "---\n",
    "\n",
    "### Late Submission Penalty\n",
    "\n",
    "Unless a Special Consideration request has been submitted and approved, a 5% penalty (of the total possible mark of the task) will be applied for each day a written report or presentation assessment is not submitted, up until the 7th day (including weekends). After the 7th day, a grade of ‘0’ will be awarded even if the assessment is submitted.\n",
    "\n",
    "> **Example:** If the assignment is worth 8 marks (of the entire unit) and your submission is late by 19 hours (or 23 hours 59 minutes 59 seconds), 0.4 marks (5% of 8 marks) will be deducted. If your submission is late by 24 hours (or 47 hours 59 minutes 59 seconds), 0.8 marks (10% of 8 marks) will be deducted, and so on.\n",
    "\n",
    "The submission time for all uploaded assessments is **11:55 PM**. A **1-hour grace period** is provided for technical concerns.  Apply for [Special Consideration](https://students.mq.edu.au/study/assessment-exams/special-consideration), if you think you should be granted an extended deadline or waive the late submission penalty. You should apply immediately when the situation occurs.\n",
    "\n",
    "---\n",
    "\n",
    "### Academic Integrity\n",
    "\n",
    "All submitted work must be your own. For rules around AI tools, refer to **\"Using Generative AI Tools\" on iLearn**.\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Complete the five tasks below.\n",
    "\n",
    "* Write your code and comments inside this notebook.\n",
    "\n",
    "* Your notebook must include the running outputs of your final code.\n",
    "\n",
    "* **Submit this `.ipynb` file, containing your code and outputs, to iLearn.**\n",
    "\n",
    "---\n",
    "\n",
    "### Assessment\n",
    "\n",
    "-  Marks are based on the correctness of your code, outputs, and coding style.\n",
    "-  A total of **2.5 marks** (0.5 per task) are awarded globally across the assignment for both of the below: (1) runnable codes; (2) good coding style: clean, modular code, meaningful variable names, and good comments.\n",
    "-  If outputs are missing or incorrect, up to **25% of the marks for that task** can be deducted.\n",
    "-  See each task below for the detailed mark breakdown.\n",
    "\n",
    "---\n",
    "\n",
    "### AI Tools Usage Policy\n",
    "\n",
    "\n",
    "In this assignment, we view AI code generators such as copilot, CodeGPT, etc as tools that can help you write code quickly. You are allowed to use these tools, but with some conditions. To understand what you can and what you cannot do, please visit these information pages provided by Macquarie University.\n",
    "\n",
    "- See: [Artificial Intelligence Tools and Academic Integrity in FSE](https://bit.ly/3uxgQP4)\n",
    "\n",
    "If you choose to use these tools, make the following explicit in your submitted file as comments starting with \"Use of AI generators in this assignment\" :\n",
    "\n",
    "- What part of your code is based on the output of such tools,\n",
    "- What tools you used,\n",
    "- What prompts you used to generate the code or text, and\n",
    "- What modifications you made on the generated code or text?\n",
    "\n",
    "This will help us assess your work fairly. \n",
    "\n",
    "**If we observe that you have used an AI generator and you do not give the above information, you may face disciplinary action.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6c533",
   "metadata": {},
   "source": [
    "\n",
    "## Objectives of this assignment\n",
    "\n",
    "In this assignment, you will work on the Quora Question Pairs (QQP) datset detailed below. The first two tasks will help you get familiar with the data, and the remaining requires you to implement deep neural networks.\n",
    "\n",
    "\n",
    "**About the Quora Question Pairs (QQP) Dataset**\n",
    "\n",
    "Description: A large dataset of 400k+ question pairs from Quora, labeled whether they are duplicates (semantically the same) or not. It features informal, noisy text with class imbalance, hard positives (low lexical overlap) and hard negatives (high overlap, different meaning). QQP is practically relevant for deduplicating FAQs, search, and support systems. Working on QQP builds transferable skills, such as text preprocessing, model comparison, threshold tuning, error analysis, and deployment-minded reasoning about real applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4113f16",
   "metadata": {},
   "source": [
    "**Get familiar with the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d3b74181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip -q install datasets    # Install the datasets package to access the dataset\n",
    "# add the packages you used, and specify the verion you installed\n",
    "\n",
    "# All required import for the assignment.\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "49aa8a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: mps\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Selecting the device to run the model on.\n",
    "cuda - Nvidia GPU\n",
    "mps - Apple M1 GPU\n",
    "cpu - CPU of the device\n",
    "'''\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "print(\"The device is:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "26cf6511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x302e46450>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 7\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c55f743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load QQP\n",
    "ds = load_dataset(\"glue\", \"qqp\")\n",
    "\n",
    "# Use validation set as our test; optionally create a smaller train subset for speed\n",
    "train_ds = ds[\"train\"]\n",
    "eval_ds  = ds[\"validation\"]\n",
    "\n",
    "q1_tr = list(train_ds[\"question1\"])\n",
    "q2_tr = list(train_ds[\"question2\"])\n",
    "y_tr  = np.array(train_ds[\"label\"])\n",
    "\n",
    "q1_te = list(eval_ds[\"question1\"])\n",
    "q2_te = list(eval_ds[\"question2\"])\n",
    "y_te  = np.array(eval_ds[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05743951",
   "metadata": {},
   "source": [
    "### Task 1. What is the top-5 common NOUN in the question1 and question2, respectively? (5 marks)\n",
    "\n",
    "Write codes that returns the top-5 common NOUN in the questions. To find the part of speech, use NLTK's \"Universal\" tag set. You may need to use NLTK's `sent_tokenize` and `word_tokenize` to get words. The function returns a list that is descendingly sorted according to freqency, e.g. [(noun1, 22), (noun2, 10), ...].\n",
    "<!-- To produce the correct results, the function must do this.  -->\n",
    "Hint: The following steps will produce the correct results:\n",
    "\n",
    "- Concatenate all questions together.\n",
    "- Use the NLTK libraries to find the tokens and the stems.\n",
    "- Use NLTK's sentence tokeniser before NLTK's word tokeniser.\n",
    "- Use NLTK's part of speech tagger, using the \"Universal\" tagset.\n",
    "- Use NLTK's `pos_tag_sents` instead of `pos_tag`.\n",
    "\n",
    "Marking Criteria: \n",
    "- 2.5 marks for the correct codes and results of each column, namely question1 and question2 columns.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "efc8fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tokens(data):\n",
    "    '''\n",
    "    Tokenizes the input data into word tokens.\n",
    "    Args:\n",
    "        data (list): A list of strings to be tokenized.\n",
    "    Returns:\n",
    "        list: A list of lists, where each sublist contains word tokens of a sentence.\n",
    "\n",
    "    Joins the list of sentences into a single string, then splits it into sentences,\n",
    "    and finally tokenizes each sentence into words.\n",
    "    '''\n",
    "    sep = ' '\n",
    "    joined_data = sep.join(data)\n",
    "    wt = [nltk.word_tokenize(s) for s in nltk.sent_tokenize(joined_data)]\n",
    "    return wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0a2d0eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "def get_stems(word_tokens):\n",
    "    '''\n",
    "    Stems the word tokens using Porter Stemmer.\n",
    "    Args:\n",
    "        word_tokens (list): A list of lists, where each sublist contains word tokens of a sentence.\n",
    "    Returns:\n",
    "        list: A list of lists, where each sublist contains stemmed word tokens of a sentence.\n",
    "    '''\n",
    "    return [[stemmer.stem(token.lower()) for token in sentence] for sentence in word_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed073470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tagged_words(word_token):\n",
    "    '''\n",
    "    Tags the word tokens with their respective parts of speech using \"universal\" tagset.\n",
    "    Args:\n",
    "        word_token (list): A list of lists, where each sublist contains word tokens of a sentence.\n",
    "    Returns:\n",
    "        list: A list of lists, where each sublist contains tuples of word tokens and their POS tags.\n",
    "    '''\n",
    "    tagged_words_list = nltk.pos_tag_sents(word_token, tagset='universal')\n",
    "    return tagged_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "bb05fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_nouns(tagged_words_list, n_top=5):\n",
    "    '''\n",
    "    Finds the most common nouns in the tagged words list.\n",
    "    Args:\n",
    "        tagged_words_list (list): A list of lists, where each sublist contains tuples of word tokens and their POS tags.\n",
    "        n_top (int): The number of most common nouns to return.\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains a noun and its frequency.\n",
    "    '''\n",
    "    tagged_nouns = [\n",
    "        stemmer.stem(tagged_word[0].lower())\n",
    "        for tagged_words in tagged_words_list\n",
    "        for tagged_word in tagged_words\n",
    "        if tagged_word[1] == 'NOUN'\n",
    "    ]\n",
    "    noun_counter = Counter(tagged_nouns)\n",
    "    return noun_counter.most_common(n_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "dfddf5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_tr_word_token = get_word_tokens(q1_tr)\n",
    "q2_tr_word_token = get_word_tokens(q2_tr)\n",
    "\n",
    "q1_tr_stem_token = get_stems(q1_tr_word_token)\n",
    "q2_tr_stem_token = get_stems(q2_tr_word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "57ef8822",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_tr_tagged_words = get_tagged_words(q1_tr_word_token)\n",
    "q2_tr_tagged_words = get_tagged_words(q2_tr_word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0659501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most common nouns in question1: [('india', 12708), ('peopl', 11507), ('way', 10598), ('differ', 8623), ('quora', 7678)]\n",
      "Top 5 most common nouns in question2: [('india', 13491), ('peopl', 12281), ('way', 11900), ('quora', 7967), ('time', 7579)]\n"
     ]
    }
   ],
   "source": [
    "most_common_nouns_in_q1 = get_most_common_nouns(q1_tr_tagged_words)\n",
    "most_common_nouns_in_q2 = get_most_common_nouns(q2_tr_tagged_words)\n",
    "\n",
    "print('Top 5 most common nouns in question1:', most_common_nouns_in_q1)\n",
    "print('Top 5 most common nouns in question2:', most_common_nouns_in_q2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ac4b3",
   "metadata": {},
   "source": [
    "### Task 2. What are the top-5 common stem 2-grams and non-stem 2-grams for question1 and question2, respectively? (5 marks)\n",
    "\n",
    "Write codes that returns the top-5 most frequent 2-grams (bigrams) of stemmed and non-stemmed tokens along with their normalized frequency from the question1 and question2 columns of the QQP dataset. The output should be in descending order of frequency, **with frequencies normalized by the total number of bigrams (rounded to 4 decimal places)**, e.g., `[(('what', 'is'), 0.0105), (('what', 'are'), 0.0053), ...]`.\n",
    "\n",
    "<!-- To produce the correct results, the function must do this: -->\n",
    "\n",
    "Hint: The following steps will produce the correct results:\n",
    "\n",
    "- Concatenate all questions together.\n",
    "- Use NLTK's sentence tokeniser before NLTK's word tokeniser.\n",
    "- Use the NLTK libraries to find the tokens and the stems.\n",
    "- Use NLTK's Porter stemmer to get the root words.\n",
    "- Round normalized frequency to 4 precision after the decimal point.\n",
    "- When computing bigrams, do not consider words that are in different sentences. For example, if we have this text: `Sentence 1. And sentence 2.` the bigrams are: `('Sentence','1'), ('1','.'), ('.','And'), ('And','sentence')`, etc. Note that the following would not be a valid bigram, since the punctuation mark and the word \"And\" are in different sentences: `('.','And')`.\n",
    "\n",
    "Marking Criteria: \n",
    "- 2.5 marks for the correct codes and restuls of each column, namely question1 and question2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_bigrams(token_sequences, top_n=5):\n",
    "    '''\n",
    "    Computes the top N most common bigrams from the token sequences using \"nltk.bigrams()\".\n",
    "    Args:\n",
    "        token_sequences (list): A list of lists, where each sublist contains word tokens of a sentence.\n",
    "        top_n (int): The number of most common bigrams to return.\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains a bigram (as a tuple of two words) and its relative frequency.\n",
    "    '''\n",
    "\n",
    "    bigram_counts = Counter()\n",
    "    total_bigrams = 0\n",
    "    for sentence in token_sequences:\n",
    "        normalized_tokens = [token.lower() for token in sentence]\n",
    "        sentence_bigrams = list(nltk.bigrams(normalized_tokens))\n",
    "        bigram_counts.update(sentence_bigrams)\n",
    "        total_bigrams += len(sentence_bigrams)\n",
    "\n",
    "    return [(bigram, round(count / total_bigrams, 4))for bigram, count in bigram_counts.most_common(top_n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "0832480b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1 top-5 non-stem bigrams: [(('what', 'is'), 0.0128), (('is', 'the'), 0.0112), (('what', 'are'), 0.0102), (('how', 'do'), 0.0084), (('can', 'i'), 0.0067)]\n",
      "Question1 top-5 stem bigrams: [(('what', 'is'), 0.0128), (('is', 'the'), 0.0112), (('what', 'are'), 0.0102), (('how', 'do'), 0.0084), (('can', 'i'), 0.0067)]\n",
      "Question2 top-5 non-stem bigrams: [(('what', 'is'), 0.0123), (('is', 'the'), 0.0108), (('what', 'are'), 0.0098), (('how', 'do'), 0.0085), (('can', 'i'), 0.0068)]\n",
      "Question2 top-5 stem bigrams: [(('what', 'is'), 0.0123), (('is', 'the'), 0.0108), (('what', 'are'), 0.0098), (('how', 'do'), 0.0085), (('can', 'i'), 0.0068)]\n"
     ]
    }
   ],
   "source": [
    "q1_top_bigrams = get_top_bigrams(q1_tr_word_token)\n",
    "q1_top_stem_bigrams = get_top_bigrams(q1_tr_stem_token)\n",
    "q2_top_bigrams = get_top_bigrams(q2_tr_word_token)\n",
    "q2_top_stem_bigrams = get_top_bigrams(q2_tr_stem_token)\n",
    "\n",
    "print('Question1 top-5 non-stem bigrams:', q1_top_bigrams)\n",
    "print('Question1 top-5 stem bigrams:', q1_top_stem_bigrams)\n",
    "\n",
    "print('Question2 top-5 non-stem bigrams:', q2_top_bigrams)\n",
    "print('Question2 top-5 stem bigrams:', q2_top_stem_bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936c3bf",
   "metadata": {},
   "source": [
    "### Task 3. Naïve Bayes Classifier (5.5 marks)\n",
    "\n",
    "The QQR dataset contains pairs of questions with labels indicating whether the two questions are semantically duplicate (1) or not (0).\n",
    "\n",
    "1. Using a Bag-of-Words representation, train a Naïve Bayes classifier to predict duplicates. (2 marks)\n",
    "\n",
    "1. Report accuracy, precision, and recall on the test set. (1.5 marks)\n",
    "\n",
    "1. Inspect your confusion matrix. Identify one type of error (false positive or false negative) that dominates. Suggest a possible reason for this pattern based on the dataset. (2 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcfe624",
   "metadata": {},
   "source": [
    "**1. Using a Bag-of-Words representation, train a Naïve Bayes classifier to predict duplicates.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "880092f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From now on, you are allowed to use a subset of the dataset which requires less computing resources.\n",
    "# Note that you have to use the same subset for the following coding tasks, which ensure fairness when comparing performance across different models.\n",
    "\n",
    "Ntrain = 1000\n",
    "Ntest = 100\n",
    "ds = load_dataset(\"glue\", \"qqp\")\n",
    "\n",
    "# Use validation set as our test; optionally create a smaller train subset for speed\n",
    "train_ds = ds[\"train\"].select(range(Ntrain))\n",
    "eval_ds  = ds[\"validation\"].select(range(Ntest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775b96b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the entire training set for fitting\n",
    "q1_train = list(train_ds['question1'])\n",
    "q2_train = list(train_ds['question2'])\n",
    "y_train = np.array(train_ds['label'])\n",
    "\n",
    "# Test set\n",
    "q1_test = list(eval_ds['question1'])\n",
    "q2_test = list(eval_ds['question2'])\n",
    "y_test = np.array(eval_ds['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b2499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using CountVectorizer from sklearn to create a bag-of-words representation for both questions.\n",
    "Taking 1500 most frequent unigrams and bigrams from the training set of question1 and question2.\n",
    "'''\n",
    "vectorizer = CountVectorizer(max_features=1500, ngram_range=(1, 2))\n",
    "vectorizer.fit(q1_train + q2_train)\n",
    "\n",
    "'''\n",
    "vectorizer.transform() -  creates a sparse matrix representation of the questions using the vocabulary of 5000.\n",
    "hstack() -  horizontally stacks the sparse matrices horizontally to create one matrix.\n",
    "This helps Naive Bayes Classifer to access both pair of questions at once to learn the relationship between them and identify if they are duplicates.\n",
    "'''\n",
    "X_train = hstack([\n",
    "    vectorizer.transform(q1_train),\n",
    "    vectorizer.transform(q2_train),\n",
    "])\n",
    "\n",
    "X_test = hstack([\n",
    "    vectorizer.transform(q1_test),\n",
    "    vectorizer.transform(q2_test),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134077f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9050\n"
     ]
    }
   ],
   "source": [
    "# Using sklearn's MultinomialNB to fit a Naive Bayes classifier on the bag-of-words representation.\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Training model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_predictions = clf.predict(X_train)\n",
    "nb_train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "print(f\"Training accuracy: {nb_train_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d8e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing model\n",
    "\n",
    "nb_confusion = confusion_matrix(y_test, test_predictions)\n",
    "nb_accuracy = accuracy_score(y_test, test_predictions)\n",
    "nb_precision = precision_score(y_test, test_predictions, zero_division=0)\n",
    "nb_recall = recall_score(y_test, test_predictions, zero_division=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d359e2a",
   "metadata": {},
   "source": [
    "**2. Report accuracy, precision, and recall on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d2209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7200\n",
      "Precision: 0.5417\n",
      "Recall: 0.4333\n",
      "Confusion Matrix:\n",
      "[[59 11]\n",
      " [17 13]]\n"
     ]
    }
   ],
   "source": [
    "# Accuracy, Precision, Recall for Test set\n",
    "print(f\"Test accuracy: {nb_accuracy:.4f}\")\n",
    "print(f\"Precision: {nb_precision:.4f}\")\n",
    "print(f\"Recall: {nb_recall:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{nb_confusion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9088d7c",
   "metadata": {},
   "source": [
    "**Analysis of Accuracy, Precision and Recall:**\n",
    "> The Naive Bayes classifier achieved a test accuracy of 72%, indicating that it correctly predicted the duplicate or non-duplicate label for most question pairs. However, the precision score of 0.54 and recall score of 0.43 reveal certain limitations in its performance. \n",
    "A precision of 0.54 suggests that only about half of the pairs predicted as duplicates were actually true duplicates, reflecting a moderate rate of false positives. The recall of 0.43 indicates that the model correctly identified less than half of the actual duplicate pairs, meaning it frequently missed true duplicates. To conclude, while the classifier performs reasonably well in distinguishing non-duplicate pairs, it struggles to capture deeper semantic similarities between questions. This is a typical issue Bag-of-Words-based models, which rely on surface-level word overlap and fail to account for contextual or paraphrased expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed019857",
   "metadata": {},
   "source": [
    "**3. Inspect your confusion matrix. Identify one type of error (false positive or false negative) that dominates. Suggest a possible reason for this pattern based on the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15d52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Visualising the confusion matrix helps explain whether false positives or false negatives dominate, \n",
    "addressing the error-analysis requirement for Task 3.\n",
    "'''\n",
    "\n",
    "# Task 3: Visualise Naive Bayes confusion matrix\n",
    "plt.figure(figsize=(5, 5))\n",
    "class_labels = ['Non-duplicate', 'Duplicate']\n",
    "sns.heatmap(\n",
    "    nb_confusion, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=class_labels, yticklabels=class_labels\n",
    ")\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Naive Bayes Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67893fe",
   "metadata": {},
   "source": [
    "**Analysis of Confusion Matrix:**\n",
    "> The confusion matrix shows that false negatives (17) outnumber false positives (11), indicating that the model more frequently fails to recognize actual duplicate question pairs. This suggests that the classifier is conservative in predicting duplicates, often labeling semantically similar questions as non-duplicates. Similar to analysis of accuracy, precision and recall, having only surface-level data and ignoring contextual or paraphrased meaning causes model to perform bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf16e2",
   "metadata": {},
   "source": [
    "### Task 4. Siamese Neural Network (7 marks)\n",
    "\n",
    "You now want to learn semantic similarity directly from the question pairs.\n",
    "\n",
    "1. Design a Siamese Neural Network with two identical LSTM encoders that embed each question. (3 marks)\n",
    "\n",
    "1. Use cosine similarity to classify duplicates, and report accuracy and F1-score. (2 marks)\n",
    "\n",
    "1. Compare your Siamese model to your Naïve Bayes model. Which one handles imbalanced errors (precision vs. recall) better in your results, and why do you think that is? (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f907f5c0",
   "metadata": {},
   "source": [
    "**1. Design a Siamese Neural Network with two identical LSTM encoders that embed each question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63294c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Tokenizer\n",
    "'''\n",
    "Using pretrained BERT tokenizer as it already has a huge vocabulary learned from large corpus.\n",
    "'''\n",
    "TOKENIZER_NAME = \"bert-base-uncased\"\n",
    "TOKENIZER = BertTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "VOCAB_SIZE = TOKENIZER.vocab_size\n",
    "PAD_TOKEN_ID = TOKENIZER.pad_token_id\n",
    "\n",
    "# Hyper parameters\n",
    "MAX_LEN = 80\n",
    "EMBED_SIZE = 256\n",
    "LSTM_UNITS = 256\n",
    "BATCH_SIZE = 25\n",
    "N_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "MARGIN = 1.0\n",
    "DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "ee5d377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDataset(Dataset):\n",
    "    '''\n",
    "    Preping data for Siamese Network.\n",
    "    Converts the sentences in to tensors for the model with data such as padding, tokenid, attention mask etc using tokenizer.encode_plus().\n",
    "    '''\n",
    "\n",
    "    def __init__(self, sents1, sents2, labels, tokenizer, max_len):\n",
    "        self.sents1 = sents1\n",
    "        self.sents2 = sents2\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Retrieves the tokenized input IDs and attention masks for both questions in the pair, along with the label.\n",
    "        Args:\n",
    "            idx (int): Index of the data point to retrieve.\n",
    "        Returns:\n",
    "            dict: A dictionary containing:\n",
    "                'ids1': Tensor of input IDs for question 1.\n",
    "                'ids2': Tensor of input IDs for question 2.\n",
    "                'labels': Tensor of the label (0 or 1). \n",
    "        '''\n",
    "        q1_text = str(self.sents1[idx])\n",
    "        q2_text = str(self.sents2[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoded1 = self.tokenizer.encode_plus(\n",
    "            q1_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoded2 = self.tokenizer.encode_plus(\n",
    "            q2_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ids1': encoded1['input_ids'].flatten(),\n",
    "            'ids2': encoded2['input_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "557ebb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    '''\n",
    "    Creating Siamese Network with shared LSTM encoders.\n",
    "    nn.Embedding(): Converts input in to vectors with randomly initialized weights.\n",
    "    nn.LSTM(): A Long Short-Term Memory (LSTM) layer for sequence modeling. Since the the entire question is already available at each time step, bidirectionality is not necessary here.\n",
    "    nn.Dropout(): Using for regularization. randomly dropping a fraction of least important features.\n",
    "\n",
    "    classifier: A feedforward neural network that takes the cosine similarity as input and outputs a logit score. \n",
    "                As we are using nn.BCEWithLogitsLoss(), we do not need to apply sigmoid activation here.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, pad_idx=PAD_TOKEN_ID, embed_dim=EMBED_SIZE, hidden_size=LSTM_UNITS, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward_once(self, tokens):\n",
    "        embedded = self.embedding(tokens)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        # Since LSTM is not bidirectional, hidden has shape (1, batch_size, hidden_size)\n",
    "        return self.dropout(hidden[0])\n",
    "        \n",
    "\n",
    "    def forward(self, q1, q2):\n",
    "        h1 = self.forward_once(q1)\n",
    "        h2 = self.forward_once(q2)\n",
    "\n",
    "        '''\n",
    "        Using cosine similarity to classify duplicates.\n",
    "        '''\n",
    "        cosine = F.cosine_similarity(h1, h2, dim=1).unsqueeze(-1)\n",
    "        logits = self.classifier(cosine).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "def train_epoch(model, loader, optimiser, criterion, device):\n",
    "    '''\n",
    "    Args:\n",
    "        model (nn.Module): The Siamese network model to be trained.\n",
    "        loader (DataLoader): DataLoader for the training data.\n",
    "        optimiser (torch.optim.Optimizer): Optimizer for updating model parameters.\n",
    "        criterion (nn.Module): Loss function to compute the loss.\n",
    "        device (torch.device): Device to run the model on (CPU or GPU).\n",
    "    Returns:\n",
    "        float: Average loss over the epoch.\n",
    "    '''\n",
    "\n",
    "    # Model in training mode as we need to update the weights using backpropagation\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in loader:\n",
    "        ids1 = batch['ids1'].to(device)\n",
    "        ids2 = batch['ids2'].to(device)\n",
    "        labels = batch['labels'].float().to(device)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        logits = model(ids1, ids2)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "bb07fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    '''\n",
    "    Args:\n",
    "        model (nn.Module): The Siamese network model to be evaluated.\n",
    "        loader (DataLoader): DataLoader for the evaluation data.\n",
    "        criterion (nn.Module): Loss function to compute the loss.\n",
    "        device (torch.device): Device to run the model on (CPU or GPU).\n",
    "    Returns:\n",
    "        tuple: A tuple containing average loss, accuracy, precision, recall, and F1-score.\n",
    "    '''\n",
    "\n",
    "    # Model in evaluation mode as we are evaluating and do not need to update weights\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_probs, all_labels = [], []\n",
    "\n",
    "    # Disable gradient calculation for evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            ids1 = batch['ids1'].to(device)\n",
    "            ids2 = batch['ids2'].to(device)\n",
    "            labels = batch['labels'].float().to(device)\n",
    "\n",
    "            logits = model(ids1, ids2)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            all_probs.extend(probs.cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    # Converting logits to probabilities to compute threshold based metrics\n",
    "    preds = (np.array(all_probs) >= 0.5).astype(float)\n",
    "    labels_np = np.array(all_labels)\n",
    "\n",
    "    accuracy = accuracy_score(labels_np, preds)\n",
    "    precision = precision_score(labels_np, preds, zero_division=0)\n",
    "    recall = recall_score(labels_np, preds, zero_division=0)\n",
    "    f1 = f1_score(labels_np, preds, zero_division=0)\n",
    "\n",
    "    return total_loss / len(loader), accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "430dc920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating datasets and dataloaders for train and test sets\n",
    "train_dataset = SiameseDataset(train_ds['question1'], train_ds['question2'], train_ds['label'], TOKENIZER, MAX_LEN)\n",
    "test_dataset = SiameseDataset(eval_ds['question1'], eval_ds['question2'], eval_ds['label'], TOKENIZER, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "125327f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss=0.7053\n",
      "Epoch 2: train loss=0.6708\n",
      "Epoch 3: train loss=0.6553\n",
      "Epoch 4: train loss=0.6509\n",
      "Epoch 5: train loss=0.6496\n"
     ]
    }
   ],
   "source": [
    "# Creating the Siamese model.\n",
    "model = SiameseNetwork().to(DEVICE)\n",
    "\n",
    "'''\n",
    "Usiing Adam optimiser becasue it provides adaptive learning rate and momentum based updates wich is suited for architechture like LSTM.\n",
    "It converges faster and more reliably than other optimisers like SGD.\n",
    "'''\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "'''\n",
    "Using BCEWithLogitsLoss because the Siamese network outputs logits that represent the similarity between two questions.\n",
    "BCEWithLogitsLoss combines a sigmoid activation and binary cross-entropy in a single, numerically stable operation, converting logits into probabilities internally. \n",
    "'''\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "'''\n",
    "Traning the Siamese model and reporting loss per epoch.\n",
    "Aftre multiple iteration, 5 epochs seemed to be most optimal for this dataset. After 5 epochs, the train loss hovers between 0.64 - 0.65 range.\n",
    "'''\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    train_loss = train_epoch(model, train_loader, optimiser, criterion, DEVICE)\n",
    "    print(f'Epoch {epoch}: train loss={train_loss:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ebc995",
   "metadata": {},
   "source": [
    "**2. Use cosine similarity to classify duplicates, and report accuracy and F1-score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "1a6d935b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "After training we compute accuracy, precision, recall, and F1.\n",
    "'''\n",
    "\n",
    "test_loss, siamese_accuracy, siamese_precision, siamese_recall, siamese_f1 = evaluate(model, test_loader, criterion, DEVICE)\n",
    "\n",
    "print(f'Accuracy: {siamese_accuracy:.4f}')\n",
    "print(f'Precision: {siamese_precision:.4f}')\n",
    "print(f'Recall: {siamese_recall:.4f}')\n",
    "print(f'F1 score: {siamese_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a3b441",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "> The Siamese LSTM network achieved a test accuracy of 70%. The precision, recall, and F1-score are all 0.0, this indicats that the model consistently predicted all pairs as non-duplicates. This suggests that the network failed to distinguish between duplicate and non-duplicate questions. This is because the predictions are below the used 0.5 threshold. The network predicts between 0.3 adnd 0.4.\n",
    "\n",
    "> Even after adjusting the hyperparamers such as initial weight of the loss function, epoch, learning rate, and cosine similarity from [-1.1] to [0,1]. the network prediction is not chagned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8691a2ef",
   "metadata": {},
   "source": [
    "**3. Compare your Siamese model to your Naïve Bayes model. Which one handles imbalanced errors (precision vs. recall) better in your results, and why do you think that is?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c7efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: Accuracy: 0.7200 | Precision: 0.5417 | Recall: 0.4333\n",
      "Siamese Network: Accuracy: 0.7000 | Precision: 0.0000 | Recall: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Naive Bayes: Accuracy: {nb_accuracy:.4f} | Precision: {nb_precision:.4f} | Recall: {nb_recall:.4f}\")\n",
    "print(f\"Siamese Network: Accuracy: {siamese_accuracy:.4f} | Precision: {siamese_precision:.4f} | Recall: {siamese_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fd4a22",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "> When comparing both the models, the Naive Bayes classifier handled imbalanced errors more effectively than the Siamese LSTM network. The Naive Bayes model achieved a test accuracy of 72%, with a precision of 0.54 and recall of 0.43, indicating that it was able to identify duplicate and non-duplicate questions with a reasonable trade off between false positives and false negatives. In contrast, the Siamese LSTM network obtained a similar accuracy of 70% but a precision and recall of 0.0, meaning it predicted all pairs as non-duplicates. This occurred because the model produced low similarity probabilities around 0.3 – 0.4 that never crossed the 0.5 classification threshold, leading to no positive (duplicate) predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf68de",
   "metadata": {},
   "source": [
    "### Task 5. Transformer-Based Classifier (10 marks)\n",
    "\n",
    "Instead of handcrafted features or LSTMs, you now fine-tune a pre-trained Transformer (e.g., BERT or RoBERTa, etc) for QQP.\n",
    "\n",
    "1. Fine-tune the model for 3 epochs with learning rate 2e-5. (3 marks)\n",
    "\n",
    "1. Report the accuracy, precision, recall, and F1-score. (2 marks)\n",
    "\n",
    "1. Compare your Transformer results with your Siamese model. Did the Transformer improve both precision and recall, or mainly one? What does this suggest about how it captures question meaning? (2 marks)\n",
    "\n",
    "1. Look at one example your Transformer misclassified. Write a short explanation of why the model might have made this mistake. (3 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d321ec",
   "metadata": {},
   "source": [
    "Task 5 switches to a Transformer encoder. We load the pretrained BERT tokenizer and record the fine-tuning hyperparameters specified in the brief.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "9eef4aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Selecting pretrained transformer model.\n",
    "'''\n",
    "TRANSFORMER_MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "'''\n",
    "Setting hyperparameters for transformer model as specified in the task\n",
    "'''\n",
    "TRANSFORMER_EPOCHS = 3\n",
    "TRANSFORMER_LR = 2e-5\n",
    "TRANSFORMER_BATCH_SIZE = 16\n",
    "TRANSFORMER_MAX_LEN = 128\n",
    "\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "8480dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_transformer(batch):\n",
    "    '''\n",
    "    Tokenizes the question pairs using the pretrained BERT tokenizer into numerical inputs for the model to train on.\n",
    "\n",
    "    Args:\n",
    "        batch (dict): A batch of data containing 'question1' and 'question2'.\n",
    "    Returns:\n",
    "        dict: A dictionary containing tokenized inputs for both questions.\n",
    "    '''\n",
    "    return transformer_tokenizer(\n",
    "        batch['question1'],\n",
    "        batch['question2'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=TRANSFORMER_MAX_LEN,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c148f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 5054.01 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 9433.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_text_pairs = list(zip(eval_ds['question1'], eval_ds['question2']))\n",
    "\n",
    "# Applies tokenize_transformer to the entire train and test datasets.\n",
    "tokenized_train = train_ds.map(\n",
    "    tokenize_transformer,\n",
    "    batched=True,\n",
    "    remove_columns=['question1', 'question2', 'idx'],\n",
    ")\n",
    "\n",
    "tokenized_test = eval_ds.map(\n",
    "    tokenize_transformer,\n",
    "    batched=True,\n",
    "    remove_columns=['question1', 'question2', 'idx'],\n",
    ")\n",
    "\n",
    "# Huggingface transformers expect the label column to be named 'labels'\n",
    "tokenized_train = tokenized_train.rename_column('label', 'labels')\n",
    "tokenized_test = tokenized_test.rename_column('label', 'labels')\n",
    "\n",
    "'''\n",
    "It ensures that \"token_type_ids\" which is segment embeddings are correctly passed to the model, allowing the transformer model to understand which question is question1 and which is question2.\n",
    "Since some models like BERT use segment embeddings to differentiate the two input questions.\n",
    "'''\n",
    "model_input_cols = ['input_ids', 'attention_mask', 'labels']\n",
    "if 'token_type_ids' in tokenized_train.features:\n",
    "    model_input_cols.insert(2, 'token_type_ids')\n",
    "\n",
    "'''\n",
    " The set_format() method ensures that each model input field is represented as a PyTorch tensor.\n",
    "'''\n",
    "tokenized_train.set_format(type='torch', columns=model_input_cols)\n",
    "tokenized_test.set_format(type='torch', columns=model_input_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dce015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We wrap the tokenised datasets in DataLoaders and instantiate the classification head from the pretrained BERT checkpoint, \n",
    "wiring up the AdamW optimiser and linear schedule required for stable fine-tuning.\n",
    "'''\n",
    "\n",
    "# Loading the data using DataLoader\n",
    "train_loader = DataLoader(tokenized_train, batch_size=TRANSFORMER_BATCH_SIZE, shuffle=True)\n",
    "eval_loader = DataLoader(tokenized_test, batch_size=TRANSFORMER_BATCH_SIZE)\n",
    "\n",
    "'''\n",
    "AutoModelForSequenceClassification.from_pretrained(): Selects \"bert-base-uncased\" model and initializes it with pretrained weights.\n",
    "num_labels=2: Modifies the final classification layer so that it outputs two logits.\n",
    "'''\n",
    "transformer_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    TRANSFORMER_MODEL_NAME,\n",
    "    num_labels=2,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(transformer_model.parameters(), lr=TRANSFORMER_LR)\n",
    "\n",
    "total_training_steps = len(train_loader) * TRANSFORMER_EPOCHS\n",
    "\n",
    "# fine tuning the model\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=max(1, int(0.1 * total_training_steps)),\n",
    "    num_training_steps=total_training_steps,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd0b27",
   "metadata": {},
   "source": [
    "**1. Fine-tune the model for 3 epochs with learning rate 2e-5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548daea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - loss: 0.6370\n",
      "Epoch 2/3 - loss: 0.4634\n",
      "Epoch 3/3 - loss: 0.3241\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "for epoch in range(1, TRANSFORMER_EPOCHS + 1):\n",
    "    transformer_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        batch = {key: value.to(DEVICE) for key, value in batch.items()}\n",
    "        outputs = transformer_model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss += loss.item()\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch}/{TRANSFORMER_EPOCHS} - loss: {avg_epoch_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0775d04",
   "metadata": {},
   "source": [
    "**2. Report the accuracy, precision, recall, and F1-score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301dc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer accuracy: 0.7700\n",
      "Transformer precision: 0.6207\n",
      "Transformer recall: 0.6000\n",
      "Transformer F1 score: 0.6102\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "transformer_model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "misclassified_idx = None\n",
    "misclassified_pred = None\n",
    "misclassified_true = None\n",
    "example_idx = 0\n",
    "\n",
    "default_eval_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        labels = batch['labels']\n",
    "        batch_on_device = {key: value.to(DEVICE) for key, value in batch.items()}\n",
    "        outputs = transformer_model(**batch_on_device)\n",
    "        default_eval_loss += outputs.loss.item()\n",
    "        batch_preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        labels_np = labels.numpy()\n",
    "        all_preds.extend(batch_preds.tolist())\n",
    "        all_labels.extend(labels_np.tolist())\n",
    "        for pred, true in zip(batch_preds, labels_np):\n",
    "            if misclassified_idx is None and pred != true:\n",
    "                misclassified_idx = example_idx\n",
    "                misclassified_pred = int(pred)\n",
    "                misclassified_true = int(true)\n",
    "            example_idx += 1\n",
    "\n",
    "transformer_accuracy = accuracy_score(all_labels, all_preds)\n",
    "transformer_precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "transformer_recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "transformer_f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "print(f'Transformer accuracy: {transformer_accuracy:.4f}')\n",
    "print(f'Transformer precision: {transformer_precision:.4f}')\n",
    "print(f'Transformer recall: {transformer_recall:.4f}')\n",
    "print(f'Transformer F1 score: {transformer_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb0fc62",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "> The fine tuned Transformer model achieved an accuracy of 77%, with a precision of 0.62, recall of 0.60, and an F1-score of 0.61. These results indicate that the model correctly classified most question pairs and maintained a balanced trade off between false positives and false negatives. Compared to earlier models, the Transformer demonstrated a stronger ability to capture contextual and semantic relationships between questions, leading to more consistent duplicate detection performance across both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b051b",
   "metadata": {},
   "source": [
    "**3. Compare your Transformer results with your Siamese model. Did the Transformer improve both precision and recall, or mainly one? What does this suggest about how it captures question meaning?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "0f33a71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuned transformer model: Accuracy: 0.7700 | Precision: 0.6207 | Recall: 0.6000\n",
      "Siamese Network: Accuracy: 0.7000 | Precision: 0.0000 | Recall: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# We contrast both models to explain precision/recall trade offs, per assignment prompt\n",
    "print(f\"Fine tuned transformer model: Accuracy: {transformer_accuracy:.4f} | Precision: {transformer_precision:.4f} | Recall: {transformer_recall:.4f}\")\n",
    "print(f\"Siamese Network: Accuracy: {siamese_accuracy:.4f} | Precision: {siamese_precision:.4f} | Recall: {siamese_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbf6e3b",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "> The fine tuned Transformer model outperformed the Siamese LSTM network in every evaluation metric, showing significant improvements in both precision and recall. While the Siamese network achieved an accuracy of 70% with precision and recall values of 0.0 which indicates that it failed to identify any duplicate pairs. The Transformer model reached an accuracy of 77%, precision of 0.62, and recall of 0.60. This improvement across both metrics suggests that the Transformer not only reduced false positives but also became much better at detecting true duplicates. The results highlight that the Transformer effectively captures semantic meaning and contextual relationships between questions. The self attention mechanism and pretrained language understanding, which allow it to recognize paraphrases and related expressions beyond simple word overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610bcd53",
   "metadata": {},
   "source": [
    "**4. Look at one example your Transformer misclassified. Write a short explanation of why the model might have made this mistake.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "aa085edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified example:\n",
      "Question 1: What are some good baby girl names starting with D?\n",
      "Question 2: What are some good baby girl names starting with D or H?\n",
      "True label: 0 - Predicted: 1\n",
      "Possible reason: The questions share almost all tokens, so the model fixates on lexical overlap and misses that the second query broadens to names starting with D or H, turning the pair into a near-duplicate despite different intent.\n"
     ]
    }
   ],
   "source": [
    "if misclassified_idx is not None:\n",
    "    mis_q1, mis_q2 = val_text_pairs[misclassified_idx]\n",
    "    print('Misclassified example:')\n",
    "    print('Question 1:', mis_q1)\n",
    "    print('Question 2:', mis_q2)\n",
    "    print(f'True label: {misclassified_true} - Predicted: {misclassified_pred}')\n",
    "    print('Possible reason: The questions share almost all tokens, so the model fixates on lexical overlap and misses that the second query broadens to names starting with D or H, turning the pair into a near-duplicate despite different intent.')\n",
    "else:\n",
    "    print('Transformer classified all evaluation examples correctly on this subset.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d67a833",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "> In this example, the Transformer model likely focused on the strong lexical and structural overlap between the two questions and treated them as semantically the same. However, the second question subtly broadens the intent by adding “or H,” which makes it a different in meaning. Because most of the tokens, order, and topic match, the model inferred high similarity and predicted “duplicate”. This suggests the model is slightly over reliant on shared wording and less sensitive to small scope changes expressed through short modifiers like “or H.”"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp3420",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
